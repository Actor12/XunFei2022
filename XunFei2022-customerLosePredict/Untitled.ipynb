{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 待做思路：\n",
    "1.画训练和测试集的特征值分布图，观察是否存在分布差异较大的特征，做剔除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-fe77d1e9ee6a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-fe77d1e9ee6a>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    data = data.apply(lambda x:11 if x[\"平均丢弃数据呼叫数\"]>10)\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#平均丢弃数据呼叫数,平均占线数据调用次数,长尾，大于10设为11\n",
    "# data = data.apply(lambda x:11 if x[\"平均丢弃数据呼叫数\"]>10)\n",
    "# data['平均占线数据调用次数'] = data['平均占线数据调用次数'].apply(lambda x:11 if x>10)\n",
    "# data['平均客户服务电话次数'] = data['平均客户服务电话次数'].apply(lambda x:51 if x>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in data.columns if f not in ['是否流失','客户ID']]\n",
    "\n",
    "train = data[data['是否流失'].notnull()].reset_index(drop=True)\n",
    "test = data[data['是否流失'].isnull()].reset_index(drop=True)\n",
    "\n",
    "x_train = train[features]\n",
    "x_test = test[features]\n",
    "\n",
    "y_train = train['是否流失']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name):\n",
    "    folds = 5\n",
    "    seed = 2022\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    train = np.zeros(train_x.shape[0])\n",
    "    test = np.zeros(test_x.shape[0])\n",
    "\n",
    "    cv_scores = []\n",
    "    test_pred_sum = np.zeros(len(test))\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************{}***{} ************************************'.format(clf_name,str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.7,\n",
    "                'bagging_fraction': 0.7,\n",
    "                'bagging_freq': 10,\n",
    "                'learning_rate': 0.2,\n",
    "                'seed': 2022,\n",
    "                'n_jobs':-1\n",
    "            }\n",
    "\n",
    "            model = clf.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=3000, early_stopping_rounds=400)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "            \n",
    "            #print(list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "                \n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'binary:logistic',\n",
    "                      'eval_metric': 'auc',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 7,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.2,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2020,\n",
    "                      'nthread': 36\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=3000, early_stopping_rounds=400)\n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(test_matrix , ntree_limit=model.best_ntree_limit)\n",
    "                 \n",
    "        if clf_name == \"cat\":\n",
    "            params = {'learning_rate': 0.2, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "                      'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "            \n",
    "            model = clf(iterations=20000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      cat_features=[], use_best_model=True, verbose=3000)\n",
    "            \n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "        \n",
    "        test_pred_sum += test_pred\n",
    "        #计算当前折验证集auc\n",
    "        #train[valid_index] = np.argmax(val_pred)\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        print(\"cur fold auc:{}\".format(roc_auc_score(val_y, val_pred)))\n",
    "        print(\"test_pred:{}\".format(test_pred))\n",
    "       \n",
    "    test = test_pred_sum/folds\n",
    "    print(test)  \n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    #print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return _,test\n",
    "    \n",
    "def lgb_model(x_train, y_train, x_test):\n",
    "    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\")\n",
    "    return lgb_train, lgb_test\n",
    "\n",
    "def xgb_model(x_train, y_train, x_test):\n",
    "    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\")\n",
    "    return xgb_train, xgb_test\n",
    "\n",
    "def cat_model(x_train, y_train, x_test):\n",
    "    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, \"cat\") \n",
    "    return cat_train, cat_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************lgb***1 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60034, number of negative: 59966\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021865 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10526\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500283 -> initscore=0.001133\n",
      "[LightGBM] [Info] Start training from score 0.001133\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[3000]\ttraining's auc: 0.999559\tvalid_1's auc: 0.81226\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.83275\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.840364\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.843659\n",
      "[15000]\ttraining's auc: 1\tvalid_1's auc: 0.844899\n",
      "Early stopping, best iteration is:\n",
      "[16920]\ttraining's auc: 1\tvalid_1's auc: 0.845641\n",
      "cur fold auc:0.8456412272046158\n",
      "test_pred:[0.20526969 0.37891549 0.49018001 ... 0.00280468 0.01868359 0.24852031]\n",
      "************************************lgb***2 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60034, number of negative: 59966\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10503\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500283 -> initscore=0.001133\n",
      "[LightGBM] [Info] Start training from score 0.001133\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[3000]\ttraining's auc: 0.999505\tvalid_1's auc: 0.806793\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.827609\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.835703\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.83948\n",
      "[15000]\ttraining's auc: 1\tvalid_1's auc: 0.841071\n",
      "Early stopping, best iteration is:\n",
      "[15379]\ttraining's auc: 1\tvalid_1's auc: 0.841228\n",
      "cur fold auc:0.8412275681713972\n",
      "test_pred:[0.29836646 0.97454817 0.52556433 ... 0.0072612  0.13010559 0.19763781]\n",
      "************************************lgb***3 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60034, number of negative: 59966\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020801 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10524\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500283 -> initscore=0.001133\n",
      "[LightGBM] [Info] Start training from score 0.001133\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[3000]\ttraining's auc: 0.999538\tvalid_1's auc: 0.811202\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.829791\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.837536\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.83999\n",
      "Early stopping, best iteration is:\n",
      "[12615]\ttraining's auc: 1\tvalid_1's auc: 0.84019\n",
      "cur fold auc:0.8401899945429319\n",
      "test_pred:[0.05854043 0.55292208 0.40404494 ... 0.00288984 0.10901022 0.47302106]\n",
      "************************************lgb***4 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60033, number of negative: 59967\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019994 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10533\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500275 -> initscore=0.001100\n",
      "[LightGBM] [Info] Start training from score 0.001100\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[3000]\ttraining's auc: 0.999526\tvalid_1's auc: 0.811926\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.834029\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.840586\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.843812\n",
      "Early stopping, best iteration is:\n",
      "[13743]\ttraining's auc: 1\tvalid_1's auc: 0.84487\n",
      "cur fold auc:0.8448697219308776\n",
      "test_pred:[0.85895404 0.18930557 0.64643755 ... 0.00281592 0.10942457 0.21278641]\n",
      "************************************lgb***5 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60033, number of negative: 59967\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10514\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500275 -> initscore=0.001100\n",
      "[LightGBM] [Info] Start training from score 0.001100\n",
      "Training until validation scores don't improve for 400 rounds\n",
      "[3000]\ttraining's auc: 0.999533\tvalid_1's auc: 0.808862\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.830204\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.837275\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.840308\n",
      "[15000]\ttraining's auc: 1\tvalid_1's auc: 0.841468\n",
      "Early stopping, best iteration is:\n",
      "[14768]\ttraining's auc: 1\tvalid_1's auc: 0.84156\n",
      "cur fold auc:0.8415602318505724\n",
      "test_pred:[0.11216495 0.71585114 0.87824175 ... 0.00253107 0.27161057 0.50361832]\n",
      "[0.30665911 0.56230849 0.58889371 ... 0.00366054 0.12776691 0.32711678]\n",
      "lgb_scotrainre_list: [0.8456412272046158, 0.8412275681713972, 0.8401899945429319, 0.8448697219308776, 0.8415602318505724]\n",
      "lgb_score_mean: 0.8426977487400791\n",
      "************************************xgb***1 ************************************\n",
      "[23:15:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.65166\teval-auc:0.64314\n",
      "[3000]\ttrain-auc:1.00082\teval-auc:0.82359\n",
      "[6000]\ttrain-auc:1.00082\teval-auc:0.82629\n",
      "[6236]\ttrain-auc:1.00082\teval-auc:0.82632\n",
      "cur fold auc:0.8264055661775832\n",
      "test_pred:[0.4702105  0.48988485 0.5604631  ... 0.03134165 0.08091743 0.5279195 ]\n",
      "************************************xgb***2 ************************************\n",
      "[23:23:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.64858\teval-auc:0.64328\n",
      "[3000]\ttrain-auc:1.00082\teval-auc:0.82075\n",
      "[6000]\ttrain-auc:1.00082\teval-auc:0.82405\n",
      "[8646]\ttrain-auc:1.00082\teval-auc:0.82491\n",
      "cur fold auc:0.8249851124402098\n",
      "test_pred:[0.16616023 0.7202229  0.6080028  ... 0.0268471  0.16728279 0.09381399]\n",
      "************************************xgb***3 ************************************\n",
      "[23:33:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.65030\teval-auc:0.63737\n",
      "[3000]\ttrain-auc:1.00082\teval-auc:0.82112\n",
      "[6000]\ttrain-auc:1.00082\teval-auc:0.82412\n",
      "[7368]\ttrain-auc:1.00082\teval-auc:0.82449\n",
      "cur fold auc:0.8245842434372959\n",
      "test_pred:[0.26999125 0.8583642  0.6682787  ... 0.04865472 0.15789907 0.657825  ]\n",
      "************************************xgb***4 ************************************\n",
      "[23:41:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.64536\teval-auc:0.63250\n",
      "[3000]\ttrain-auc:1.00084\teval-auc:0.82445\n",
      "[6000]\ttrain-auc:1.00084\teval-auc:0.82785\n",
      "[9000]\ttrain-auc:1.00084\teval-auc:0.82875\n",
      "[10667]\ttrain-auc:1.00084\teval-auc:0.82902\n",
      "cur fold auc:0.8290604206839737\n",
      "test_pred:[0.41751552 0.2733318  0.7923149  ... 0.07118481 0.05441161 0.48694357]\n",
      "************************************xgb***5 ************************************\n",
      "[23:54:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.64663\teval-auc:0.63727\n",
      "[3000]\ttrain-auc:1.00084\teval-auc:0.81800\n",
      "[6000]\ttrain-auc:1.00084\teval-auc:0.82114\n",
      "[7726]\ttrain-auc:1.00084\teval-auc:0.82161\n",
      "cur fold auc:0.8216992824784084\n",
      "test_pred:[0.12630554 0.5282937  0.6566163  ... 0.03856027 0.15796928 0.24686357]\n",
      "[0.2900366  0.5740195  0.65713514 ... 0.04331771 0.12369604 0.40267312]\n",
      "xgb_scotrainre_list: [0.8264055661775832, 0.8249851124402098, 0.8245842434372959, 0.8290604206839737, 0.8216992824784084]\n",
      "xgb_score_mean: 0.8253469250434943\n"
     ]
    }
   ],
   "source": [
    "lgb_train, lgb_test = lgb_model(x_train, y_train, x_test)\n",
    "xgb_train, xgb_test = xgb_model(x_train, y_train, x_test)\n",
    "#cat_train, cat_test = cat_model(x_train, y_train, x_test)\n",
    "final_test = (lgb_test + xgb_test) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.30665911, 0.56230849, 0.58889371, ..., 0.00366054, 0.12776691,\n",
       "        0.32711678]),\n",
       " array([0.2900366 , 0.5740195 , 0.65713514, ..., 0.04331771, 0.12369604,\n",
       "        0.40267312]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_test,xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['是否流失'] = final_test\n",
    "test[['客户ID','是否流失']].to_csv('./data/test_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['是否流失'] = lgb_test\n",
    "test[['客户ID','是否流失']].to_csv('./data/lgb_test_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['是否流失'] = xgb_test\n",
    "test[['客户ID','是否流失']].to_csv('./data/xgb_test_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['是否流失'] = cat_test\n",
    "test[['客户ID','是否流失']].to_csv('./data/cat_test_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
